{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmiriShavaki/KG-based-RAG-for-Multi-hop-QA/blob/main/src/Construct_Knowledge_Graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install requirements"
      ],
      "metadata": {
        "id": "BmZG-NvPc1Zd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKTD8HXdodF9",
        "outputId": "efb6cb2d-c121-4702-fa9c-44ba893e9f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.10/dist-packages (5.24.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.45.0)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.24)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.39)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.120)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2024.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain neo4j openai wikipedia tiktoken langchain_openai langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to the database"
      ],
      "metadata": {
        "id": "ypK3kQyZc6N7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NpLG0KJqrN2T"
      },
      "outputs": [],
      "source": [
        "from langchain.graphs import Neo4jGraph\n",
        "\n",
        "url = input(\"Enter your Neo4j URL: \")\n",
        "\n",
        "username = input(\"Enter your Neo4j username: \")\n",
        "password = input(\"Enter your Neo4j password: \")\n",
        "\n",
        "graph = Neo4jGraph(\n",
        "    url=url,\n",
        "    username=username,\n",
        "    password=password\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Delete the graph if needed (uncomment before run)"
      ],
      "metadata": {
        "id": "tphLrCAtnURd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# graph.query(\"MATCH (n) DETACH DELETE n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4kYM0tRnTH0",
        "outputId": "9d39e0b1-5442-4380-df44-11af1c2f8077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge graph data structure definition"
      ],
      "metadata": {
        "id": "UTMTtFzekBy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN_-ZQx5wAzw"
      },
      "outputs": [],
      "source": [
        "from langchain_community.graphs.graph_document import (\n",
        "    Node as BaseNode,\n",
        "    Relationship as BaseRelationship,\n",
        "    GraphDocument,\n",
        ")\n",
        "from langchain.schema import Document\n",
        "from typing import List, Dict, Any, Optional\n",
        "from langchain.pydantic_v1 import Field, BaseModel\n",
        "\n",
        "class Property(BaseModel):\n",
        "  \"\"\"A single property consisting of key and value\"\"\"\n",
        "  key: str = Field(..., description=\"key\")\n",
        "  value: str = Field(..., description=\"value\")\n",
        "\n",
        "class Node(BaseNode):\n",
        "    properties: Optional[List[Property]] = Field(\n",
        "        None, description=\"List of node properties\")\n",
        "\n",
        "class Relationship(BaseRelationship):\n",
        "    properties: Optional[List[Property]] = Field(\n",
        "        None, description=\"List of relationship properties\"\n",
        "    )\n",
        "\n",
        "class KnowledgeGraph(BaseModel):\n",
        "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
        "    nodes: List[Node] = Field(\n",
        "        ..., description=\"List of nodes in the knowledge graph\")\n",
        "    rels: List[Relationship] = Field(\n",
        "        ..., description=\"List of relationships in the knowledge graph\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility mapping functions"
      ],
      "metadata": {
        "id": "xmTcPHr7l6fA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHlnW-Cnw_R-"
      },
      "outputs": [],
      "source": [
        "def format_property_key(s: str) -> str:\n",
        "    words = s.split()\n",
        "    if not words:\n",
        "        return s\n",
        "    first_word = words[0].lower()\n",
        "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
        "    return \"\".join([first_word] + capitalized_words)\n",
        "\n",
        "def props_to_dict(props) -> dict:\n",
        "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
        "    properties = {}\n",
        "    if not props:\n",
        "      return properties\n",
        "    for p in props:\n",
        "        properties[format_property_key(p.key)] = p.value\n",
        "    return properties\n",
        "\n",
        "def map_to_base_node(node: Node) -> BaseNode:\n",
        "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
        "    properties = props_to_dict(node.properties) if node.properties else {}\n",
        "    properties[\"name\"] = node.id.title()\n",
        "    return BaseNode(\n",
        "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
        "    )\n",
        "\n",
        "\n",
        "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
        "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
        "    source = map_to_base_node(rel.source)\n",
        "    target = map_to_base_node(rel.target)\n",
        "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
        "    return BaseRelationship(\n",
        "        source=source, target=target, type=rel.type, properties=properties\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The prompt and LLM configuration"
      ],
      "metadata": {
        "id": "_XMgpEOv2A8x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg9sogyvyG8o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.chains.openai_functions import (\n",
        "    create_openai_fn_chain,\n",
        "    create_structured_output_chain,\n",
        ")\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = password = input(\"Enter your OpenAI API Key: \")\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "def get_extraction_chain(\n",
        "    allowed_nodes: Optional[List[str]] = None,\n",
        "    allowed_rels: Optional[List[str]] = None\n",
        "    ):\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [(\n",
        "          \"system\",\n",
        "          f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
        "## 1. Overview\n",
        "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
        "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
        "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
        "## 2. Labeling Nodes\n",
        "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
        "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
        "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
        "{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
        "{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
        "## 3. Handling Numerical Data and Dates\n",
        "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
        "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
        "- **Property Format**: Properties must be in a key-value format.\n",
        "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
        "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
        "## 4. Coreference Resolution\n",
        "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
        "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"),\n",
        "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.\n",
        "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial.\n",
        "## 5. Strict Compliance\n",
        "Adhere to the rules strictly. Non-compliance will result in termination.\n",
        "          \"\"\"),\n",
        "            (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
        "            (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
        "        ])\n",
        "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extract_and_store_graph function definition"
      ],
      "metadata": {
        "id": "-du1WHhbi5WS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV8IDqIPyoZn"
      },
      "outputs": [],
      "source": [
        "def extract_and_store_graph(\n",
        "    document: Document,\n",
        "    nodes:Optional[List[str]] = None,\n",
        "    rels:Optional[List[str]]=None) -> None:\n",
        "\n",
        "    extract_chain = get_extraction_chain(nodes, rels)\n",
        "    data = extract_chain.invoke(document.page_content)['function']\n",
        "\n",
        "    graph_document = GraphDocument(\n",
        "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
        "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
        "      source = document\n",
        "    )\n",
        "\n",
        "    graph.add_graph_documents([graph_document])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset documents"
      ],
      "metadata": {
        "id": "_erpIZktmOkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5xCPEKVypsx"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "def read_documents_from_csv(filename, document_class=Document):\n",
        "  \"\"\"\n",
        "  Reads page_contents from a CSV file and creates a list of documents, ensuring unique entries.\n",
        "\n",
        "  Args:\n",
        "      filename: Path to the CSV file.\n",
        "      document_class: Class used to create document objects (defaults to Document).\n",
        "\n",
        "  Returns:\n",
        "      A list of Document objects with unique page_content.\n",
        "  \"\"\"\n",
        "  unique_contents = defaultdict(int)\n",
        "  documents = []\n",
        "\n",
        "  with open(filename, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    next(reader, None)\n",
        "    for row in reader:\n",
        "      text = row[0]\n",
        "      if unique_contents[text] == 0:\n",
        "        documents.append(document_class(page_content=text))\n",
        "      unique_contents[text] += 1\n",
        "\n",
        "  return documents\n",
        "\n",
        "raw_documents = read_documents_from_csv('shuffled_dataset.csv')\n",
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=2048, chunk_overlap=24)\n",
        "\n",
        "documents = text_splitter.split_documents(raw_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run extract_and_store_graph on documents"
      ],
      "metadata": {
        "id": "1Fo8I_BO-G_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdeTg1371JJy",
        "outputId": "f6300317-f914-40fc-8ddc-bb83711c64b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 498/498 [15:17<00:00,  1.84s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "allowed_nodes = [\"Club\", \"Player\", \"Trophy\", \"Stadium\", \"Number\", \"Document\", \"Date\", \"Aircraft\", \"Animal\", \"Diet\", \"Airline\", \"Programming Language\", \"Company\", \"Novel\", \"Writer\", \"Prize\", \"Theme\", \"Building\", \"Material\"]\n",
        "allowed_relations = [\"Won\", \"Signed\", \"Transferred to\", \"Joined\", \"Shirt colour\", \"Home stadium of\", \"Had jersey number of\", \"Located in\", \"Where was signed\", \"Has captained\", \"Had role of\", \"Architectural style of\", \"Largest operator in\", \"Used by\", \"Operates over\", \"Diet consists of\", \"Largest land animal found in\", \"Main theme of\", \"Tallest in\", \"Used in\"]\n",
        "\n",
        "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
        "    try:\n",
        "        extract_and_store_graph(d, allowed_nodes, allowed_relations)\n",
        "    except:\n",
        "        print(\"Error:\", d)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Write all relations to a text file"
      ],
      "metadata": {
        "id": "Y7kATeik_WTR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa-QpgWRec04",
        "outputId": "065ccae1-e514-4eeb-a05f-dd188d00b587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error on  {'n': {'name': 'Megan Rapinoe', 'id': 'Megan Rapinoe'}, 'r': ({'name': 'Megan Rapinoe', 'id': 'Megan Rapinoe'}, 'HAD_ROLE_OF', {'id': 'Left-Sided Wide Forward'}), 'm': {'id': 'Left-Sided Wide Forward'}}\n",
            "Error on  {'n': {'name': 'Megan Rapinoe', 'id': 'Megan Rapinoe'}, 'r': ({'name': 'Megan Rapinoe', 'id': 'Megan Rapinoe'}, 'HAS_CAPTAINED', {'id': 'Team'}), 'm': {'id': 'Team'}}\n",
            "Error on  {'n': {'id': 'Woollymammoth'}, 'r': ({'id': 'Woollymammoth'}, 'LARGEST_LAND_ANIMAL_FOUND_IN', {'id': 'Siberia'}), 'm': {'id': 'Siberia'}}\n",
            "Error on  {'n': {'id': 'Woollymammoth'}, 'r': ({'id': 'Woollymammoth'}, 'LARGEST_LAND_ANIMAL_FOUND_IN', {'id': 'Northamerica'}), 'm': {'id': 'Northamerica'}}\n",
            "Error on  {'n': {'name': 'Mayflower', 'id': 'Mayflower'}, 'r': ({'name': 'Mayflower', 'id': 'Mayflower'}, 'OPERATES_OVER', {'id': 'North America'}), 'm': {'id': 'North America'}}\n",
            "Error on  {'n': {'id': 'Tyrannosaurus Rex'}, 'r': ({'id': 'Tyrannosaurus Rex'}, 'DISCOVERED_IN', {'id': 'North America'}), 'm': {'id': 'North America'}}\n",
            "Error on  {'n': {'name': 'Bakelite', 'id': 'Bakelite', 'firstsyntheticplastic': 'true'}, 'r': ({'name': 'Bakelite', 'id': 'Bakelite', 'firstsyntheticplastic': 'true'}, 'COMPOSED_OF', {'id': 'Phenol'}), 'm': {'id': 'Phenol'}}\n",
            "Error on  {'n': {'name': 'Bakelite', 'id': 'Bakelite', 'firstsyntheticplastic': 'true'}, 'r': ({'name': 'Bakelite', 'id': 'Bakelite', 'firstsyntheticplastic': 'true'}, 'COMPOSED_OF', {'id': 'Formaldehyde'}), 'm': {'id': 'Formaldehyde'}}\n",
            "Error on  {'n': {'name': 'Argentina', 'id': 'Argentina'}, 'r': ({'name': 'Argentina', 'id': 'Argentina'}, 'SHIRT_COLOUR', {'id': 'Light Blue And White'}), 'm': {'id': 'Light Blue And White'}}\n",
            "Error on  {'n': {'id': 'Eagles'}, 'r': ({'id': 'Eagles'}, 'SHIRT_COLOUR', {'id': 'Midnight Green'}), 'm': {'id': 'Midnight Green'}}\n",
            "Error on  {'n': {'name': 'California Condor', 'id': 'California Condor'}, 'r': ({'name': 'California Condor', 'id': 'California Condor'}, 'LARGEST_LAND_ANIMAL_FOUND_IN', {'id': 'North America'}), 'm': {'id': 'North America'}}\n",
            "Error on  {'n': {'id': 'England'}, 'r': ({'id': 'England'}, 'WON', {'id': 'Rugby World Cup'}), 'm': {'id': 'Rugby World Cup'}}\n",
            "Error on  {'n': {'id': 'Great White Shark'}, 'r': ({'id': 'Great White Shark'}, 'LARGEST_PREDATOR_IN', {'id': 'Atlantic Ocean'}), 'm': {'id': 'Atlantic Ocean'}}\n",
            "Error on  {'n': {'id': 'Miami Heat'}, 'r': ({'id': 'Miami Heat'}, 'SHIRT_COLOUR', {'id': 'Black'}), 'm': {'id': 'Black'}}\n",
            "Error on  {'n': {'id': 'Miami Heat'}, 'r': ({'id': 'Miami Heat'}, 'SHIRT_COLOUR', {'id': 'Red'}), 'm': {'id': 'Red'}}\n",
            "Error on  {'n': {'id': 'Miami Heat'}, 'r': ({'id': 'Miami Heat'}, 'SHIRT_COLOUR', {'id': 'White'}), 'm': {'id': 'White'}}\n",
            "Error on  {'n': {'id': 'Inca Civilization'}, 'r': ({'id': 'Inca Civilization'}, 'MAIN_THEME_OF', {'id': 'Agriculture'}), 'm': {'id': 'Agriculture'}}\n",
            "Error on  {'n': {'id': 'Inca Civilization'}, 'r': ({'id': 'Inca Civilization'}, 'MAIN_THEME_OF', {'id': 'Trade'}), 'm': {'id': 'Trade'}}\n",
            "Error on  {'n': {'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'r': ({'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'DIET_CONSISTS_OF', {'id': 'Grass'}), 'm': {'id': 'Grass'}}\n",
            "Error on  {'n': {'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'r': ({'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'DIET_CONSISTS_OF', {'id': 'Leaves'}), 'm': {'id': 'Leaves'}}\n",
            "Error on  {'n': {'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'r': ({'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'DIET_CONSISTS_OF', {'id': 'Bark'}), 'm': {'id': 'Bark'}}\n",
            "Error on  {'n': {'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'r': ({'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'DIET_CONSISTS_OF', {'id': 'Fruit'}), 'm': {'id': 'Fruit'}}\n",
            "Error on  {'n': {'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'r': ({'name': 'Africanelephants', 'id': 'Africanelephants', 'diet': 'herbivore', 'dailyfoodintake': '350 pounds of vegetation'}, 'DIET_CONSISTS_OF', {'id': 'Foliage'}), 'm': {'id': 'Foliage'}}\n",
            "Error on  {'n': {'name': 'Argentinosaurus', 'id': 'Argentinosaurus', 'diet': 'herbivorous'}, 'r': ({'name': 'Argentinosaurus', 'id': 'Argentinosaurus', 'diet': 'herbivorous'}, 'DIET_CONSISTS_OF', {'id': 'Plant'}), 'm': {'id': 'Plant'}}\n",
            "Error on  {'n': {'id': 'Time'}, 'r': ({'id': 'Time'}, 'PRIZE', {'id': 'Andy Grove'}), 'm': {'id': 'Andy Grove'}}\n",
            "Error on  {'n': {'id': 'Andy Grove'}, 'r': ({'id': 'Andy Grove'}, 'HAD_ROLE_OF', {'id': 'Intel'}), 'm': {'id': 'Intel'}}\n",
            "Error on  {'n': {'id': 'First United States Congress'}, 'r': ({'id': 'First United States Congress'}, 'LOCATED_IN', {'id': 'Federal Hall'}), 'm': {'id': 'Federal Hall'}}\n",
            "Error on  {'n': {'id': 'Pyeongchang'}, 'r': ({'id': 'Pyeongchang'}, 'PRIMARY_ECONOMIC_ACTIVITY', {'id': 'Tourism'}), 'm': {'id': 'Tourism'}}\n",
            "Error on  {'n': {'name': \"Hôtel D'York\", 'architecturalstyle': 'classical', 'location': 'Paris', 'id': \"Hôtel D'York\"}, 'r': ({'name': \"Hôtel D'York\", 'architecturalstyle': 'classical', 'location': 'Paris', 'id': \"Hôtel D'York\"}, 'ARCHITECTURAL_STYLE_OF', {'id': 'Classical'}), 'm': {'id': 'Classical'}}\n"
          ]
        }
      ],
      "source": [
        "data = graph.query(\"Match (n)-[r]->(m) Return n,r,m\")\n",
        "\n",
        "with open('relations.txt', 'w', encoding='utf-8') as file:\n",
        "    for item in data:\n",
        "        try:\n",
        "            n = item['n']['name']\n",
        "            r = item['r'][1]\n",
        "            m = item['m']['name']\n",
        "            file.write(f\"{n}\\t{r}\\t{m}\\n\")\n",
        "        except:\n",
        "            print(\"Error on \", item)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BmZG-NvPc1Zd",
        "ypK3kQyZc6N7",
        "UTMTtFzekBy3",
        "xmTcPHr7l6fA"
      ],
      "authorship_tag": "ABX9TyMHmtF9AUKqKLwML8aI+ajM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}